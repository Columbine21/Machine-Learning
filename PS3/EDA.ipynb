{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 探索性数据分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次实验要求我们利用给定的8000个英文名字，训练一个循环神经网络，完成对于任意给定的名字前缀生成（补全）给定的名字。为了更好的完成我们的任务，我们应首先考虑对数据集进行必要的分析："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Dataset items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "def build_dataset(files_list=['data/female.txt',\n",
    "                              'data/male.txt']) -> List[str]:\n",
    "    dataset = []\n",
    "    for file_path in files_list:\n",
    "        with open(file_path) as file:\n",
    "            for line in file.readlines():\n",
    "                if not line.startswith('# ') and len(line.strip()) > 0:\n",
    "                    dataset.append(line.strip().lower())\n",
    "    return dataset\n",
    "\n",
    "nameSet = build_dataset()\n",
    "print(len(nameSet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## show the most common names & their frequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Gale', 3), ('Abbey', 2), ('Abbie', 2), ('Abby', 2), ('Addie', 2), ('Adrian', 2), ('Adrien', 2), ('Ajay', 2), ('Alex', 2), ('Alexis', 2), ('Alfie', 2), ('Ali', 2), ('Alix', 2), ('Allie', 2), ('Allyn', 2), ('Andie', 2), ('Andrea', 2), ('Andy', 2), ('Angel', 2), ('Angie', 2), ('Ariel', 2), ('Ashley', 2), ('Aubrey', 2), ('Augustine', 2), ('Austin', 2), ('Averil', 2), ('Barrie', 2), ('Barry', 2), ('Beau', 2), ('Bennie', 2), ('Benny', 2), ('Bernie', 2), ('Bert', 2), ('Bertie', 2), ('Bill', 2), ('Billie', 2), ('Billy', 2), ('Blair', 2), ('Blake', 2), ('Bo', 2), ('Bobbie', 2), ('Bobby', 2), ('Brandy', 2), ('Brett', 2), ('Britt', 2), ('Brook', 2), ('Brooke', 2), ('Brooks', 2), ('Bryn', 2), ('Cal', 2)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "fdist = FreqDist(nameSet)\n",
    "tops=fdist.most_common(50)\n",
    "\n",
    "\n",
    "print(tops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the character set \n",
    "\n",
    "This Part aims at making sure the size of our character set & show them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ann-Mari\n",
      "Ann-Marie\n",
      "Anna-Diana\n",
      "Anna-Diane\n",
      "Anna-Maria\n",
      "Anne-Corinne\n",
      "Anne-Mar\n",
      "Anne-Marie\n",
      "Barbara-Anne\n",
      "Bette-Ann\n",
      "Carol-Jean\n",
      "Dee Dee\n",
      "Diane-Marie\n",
      "E'Lane\n",
      "Helen-Elizabeth\n",
      "Holly-Anne\n",
      "Jo Ann\n",
      "Jo-Ann\n",
      "Jo-Anne\n",
      "Kara-Lynn\n",
      "Marie-Ann\n",
      "Marie-Jeanne\n",
      "Paula-Grace\n",
      "Sara-Ann\n",
      "Sheila-Kathryn\n",
      "Sue-elle\n",
      "Terri-Jo\n",
      "Theresa-Marie\n",
      "Zsa Zsa\n",
      "Hans-Peter\n",
      "Jean-Christophe\n",
      "Jean-Francois\n",
      "Jean-Lou\n",
      "Jean-Luc\n",
      "Jean-Marc\n",
      "Jean-Paul\n",
      "Jean-Pierre\n",
      "John-David\n",
      "John-Patrick\n",
      "{'D', 'M', 'L', 'A', ' ', 'J', 'W', 'c', 'n', 'K', 'b', 'j', 'U', 'P', 'l', 'z', 'w', 'x', 'o', 'N', 'r', 'y', 'Z', 'B', 'e', 'Q', 'T', 'I', 'a', 'p', 'R', 'k', 'Y', 'v', 'E', 'i', 'u', \"'\", 'F', '-', 's', 'q', 'O', 'S', 'g', 'd', 'C', 't', 'V', 'X', 'm', 'h', 'f', 'G', 'H'}\n"
     ]
    }
   ],
   "source": [
    "character_set = set()\n",
    "for word in nameSet:\n",
    "    if word.find(\" \") != -1 or word.find(\"'\") != -1 or word.find(\"-\") != -1:\n",
    "        print(word)\n",
    "    character_set.update(word)\n",
    "print(character_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', \"'\", '-', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(character_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the longgest name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_length = [(len(word), word) for word in nameSet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_length, reverse=True)[0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
